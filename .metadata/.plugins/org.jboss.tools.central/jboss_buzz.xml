<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Eliminate downtime during OpenShift rolling updates</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/08/eliminate-downtime-during-openshift-rolling-updates" /><author><name>Rupesh Patel</name></author><id>97afecef-2ffb-499a-80dc-28374c1a1891</id><updated>2022-06-08T07:00:00Z</updated><published>2022-06-08T07:00:00Z</published><summary type="html">&lt;p&gt;Do your clients complain about interruptions during software upgrades? Do you observe connection failures or timeouts during those upgrades? In this article, you'll learn how you can minimize the impacts on your client visiting your services hosted on the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; during software updates.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;rolling update&lt;/em&gt; creates new pods running the new software and terminates old ones. The deployment controller performs this rollout incrementally, ensuring that a certain number of new pods are ready before the controller deletes the old pods that the new pods are replacing. For details, see &lt;a href="https://docs.openshift.com/container-platform/4.10/applications/deployments/deployment-strategies.html#deployments-rolling-strategy_deployment-strategies"&gt;Rolling strategy&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; documentation.&lt;/p&gt; &lt;p&gt;Figure 1 shows a typical sequence of events during an update. The important point, for the purposes of this article, is that pods go through a transitional period where they are present but not functional. Achieving a zero-downtime rollout requires some care to drain traffic from the old pods and allow the OpenShift router time to update its configuration before the deployment controller removes the old pods.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%20from%202022-04-26%2016-18-09.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screenshot%20from%202022-04-26%2016-18-09.png?itok=5x5Q9bwB" width="868" height="727" alt="A rolling update adds and removes pods, while the service points to both old and new pods." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A rolling update adds and removes pods, while the service points to both old and new pods. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: A rolling update adds and removes pods.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Pod termination starts with setting its &lt;code&gt;deletionTimestamp&lt;/code&gt; field to a non-null value to indicate that it has been marked for deletion. An &lt;code&gt;oc get&lt;/code&gt; or &lt;code&gt;kubectl get&lt;/code&gt; command shows such a pod in a &lt;code&gt;Terminating&lt;/code&gt; state. A pod may exist in this state for some period of time (several seconds or minutes, possibly even hours or days) before the pod is actually removed. See &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination"&gt;Termination of Pods&lt;/a&gt; in the Kubernetes documentation for details.&lt;/p&gt; &lt;p&gt;When a pod enters the &lt;code&gt;Terminating&lt;/code&gt; state, different parts of the system react to resolve the transitional status:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;The kubelet updates the pod's status to &lt;code&gt;Ready&lt;/code&gt;=&lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The endpoint slice controller observes the update to the pod's status and removes the pod's IP address from any &lt;code&gt;EndpointSlice&lt;/code&gt; object that has it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The OpenShift router observes this update to the &lt;code&gt;EndpointSlice&lt;/code&gt;. The router removes the pod's IP address from the &lt;a href="https://github.com/haproxy"&gt;HAProxy&lt;/a&gt; configuration to stop HAProxy from forwarding requests to the pod. Finally, the router reloads HAProxy so that the configuration changes take effect.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thus, there can be a delay between when the pod is marked for deletion and when the OpenShift router reloads HAProxy with the updated configuration.&lt;/p&gt; &lt;p&gt;How does this affect the risk of downtime? Suppose you have a route with some backend pod, and a client sends a request for that route. Any of the following can happen with that request:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;HAProxy forwards the request to the backend pod, and it remains responsive for the duration of the transaction. In this case, the pod sends a response, and HAProxy forwards the response to the client. Everything is fine.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;HAProxy forwards the request to the backend pod, and the pod is terminated during the transaction. In this case, HAProxy returns an error response to the client. This makes the service appear to be down, even though many other pods are running.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;HAProxy forwards the request to a backend pod that has already been terminated. In this case, the connection to the pod fails. Then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;If there is no other backend pod, HAProxy returns an error response to the client.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If there is another backend pod, HAProxy retries the request with that pod. In this case, the client gets a successful response, although it might be delayed while HAProxy's connection to the first backend pod fails and HAProxy retries the request with the other pod.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Solution: A PreStop container hook&lt;/h2&gt; &lt;p&gt;The risk of downtime can be almost completely eliminated through a simple solution: the introduction of an arbitrary delay during the &lt;code&gt;Terminating&lt;/code&gt; state so that a pod continues to accept and handle requests until HAProxy stops forwarding requests to that pod. This grace period can be added by adding a &lt;code&gt;PreStop&lt;/code&gt; hook to the deployment.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;PreStop&lt;/code&gt; hook simply delays pod termination in order to allow HAProxy to stop forwarding requests to it. In addition, if the application handles long-lived connections, the &lt;code&gt;PreStop&lt;/code&gt; hook must delay the pod's removal long enough for these connections to finish.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The application process itself may have a built-in termination grace period. In this case, adding a &lt;code&gt;PreStop&lt;/code&gt; hook would be superfluous.&lt;/p&gt; &lt;p&gt;If the application doesn't have long-lived connections, 15 to 30 seconds should be plenty of time for the &lt;code&gt;PreStop&lt;/code&gt; hook. The administrator should test the &lt;code&gt;PreStop&lt;/code&gt; hook with different values and set a value that suits their environment. It is crucial that the pod continues to respond to requests while it is in the &lt;code&gt;Terminating&lt;/code&gt; state.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The administrator should keep in mind that adding a &lt;code&gt;PreStop&lt;/code&gt; hook consumes more time for recycling pods than usual.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Graceful termination requires time. Rolling updates can take up to several minutes to complete. For certain applications, graceful termination doesn't provide value. Determining whether it is worthwhile, and how long the grace period needs to be to allow traffic to drain, is up to the administrator. When configured appropriately, graceful termination can improve the experience for your end users.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/08/eliminate-downtime-during-openshift-rolling-updates" title="Eliminate downtime during OpenShift rolling updates"&gt;Eliminate downtime during OpenShift rolling updates&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rupesh Patel</dc:creator><dc:date>2022-06-08T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.22.1 released!</title><link rel="alternate" href="https://blog.kie.org/2022/06/kogito-1-22-1-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/06/kogito-1-22-1-released.html</id><updated>2022-06-08T01:23:46Z</updated><content type="html">We are glad to announce that the Kogito 1.22.1 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * Added support for models including collections at Data Index service * Support to access HTTP headers in Serverless Workflows started using REST. * Debug logger for processes in quarkus dev mode BREAKING CHANGES * Codegen maven step should be disable or there is a likely chance openapi in Serverless Workflow will stop working (Quarkiverse integration is enabled with codegen maven step and is still in experimental phase, some specs are working, but others not) * To disable this experimental change, please remove the maven goals “generate-code” and “generate-code-tests” from the Quarkus Maven Plugin in your pom.xml file. This will keep the old OpenAPI integration feature in place.  If you want to use the experimental feature, please add the goals mentioned above to your pom.xml file. The properties now must be changed to "quarkus.rest-client.&lt;class fqdn&gt;.url". The class FQDN can be found in "target/generated-code/open-api-stream". In the package "api" you will find the generated REST stubs that you need to use to properly configure the extension. Please for more information and the . In the next version we will support version (also see the ), which will favor the Quarkus REST Client integration and will make it easy to properly configure this integration.  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.19.0 artifacts are available at the . A detailed changelog for 1.22.0 can be found in as well as for . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Kafka Distributions Landscape</title><link rel="alternate" href="http://www.ofbizian.com/2022/06/kafka-distributions-landscape.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/06/kafka-distributions-landscape.html</id><updated>2022-06-07T10:19:00Z</updated><content type="html">One aspect of Apache Kafka that makes it superior to other event streaming projects is not its technical features and performance characteristics, but the ecosystem that surrounds it. The number of books, courses, conference talks, Kafka service providers, consultancies, independent professionals, third-party tools and developer libraries that make up the Kafka landscape is unmatched by competing projects. While this makes Kafka a de facto standard for event streaming and provides assurance that it will be around for a long time to come, at the same time, Kafka alone is just a cog in the wheel and does not solve business problems on its own. This raises the question of which Kafka distributions are best suited to our use cases and which ecosystem will enable the highest productivity for our development teams and organizational constraints. In this post, we will try to navigate the growing ecosystem of Kafka distributions and give you some thoughts on where the industry is heading. KAFKA FOR LOCAL DEVELOPMENT If you are new to Kafka, you may assume that all you need is a Kafka cluster, and you are done. While this statement might be correct for organizations with a low level of Kafka adoption where Kafka is a generic messaging infrastructure, it does not represent the picture in the organizations with a higher level of event-streaming adoption where Kafka is used heavily by multiple teams in multiple sophisticated scenarios. The latter group needs developer productivity tools that offer rapid feedback during development of event-driven applications, high levels of automation, and repeatability in lower environments, and depending on the business priorities a variety of hybrid deployment mechanisms from edge to multiple clouds in production. The very first thing a developer team working heavily with stream processing applications would want is being able to start a short-lived Kafka quickly on their laptop. That is true regardless if you practice test-driven development and mock all external dependencies, or a rapid prototyping technique. As a developer, I want to quickly validate that my application is wiring up and functioning properly with an in-memory database or messaging system. Then I want to be able to do repeatable integration testing with a real Kafka broker. Having this rapid feedback cycle enables developers to iterate and deliver features faster and adapt to changing requirements. The good news is that there are a few projects addressing this need. The ones that I’m most familiar with are and from Spring in the Java ecosystem. The easiest way to unit test Kafka applications is with smallrye-messaging that replaces the channel implementation with . This has nothing to do with Kafka, but shows how using the right streaming abstraction libraries can help you unit test applications rapidly. Another option is to start an in-memory Kafka cluster in the same process as the test resource through to use that for a quick integration test. If you want to start a real Kafka broker as a separate process as part of the resource, Quarkus can do that through Dev Services for Kafka. With this mechanism, Quarkus will start a Kafka cluster in less than a second using containers. This mechanism can validate Kafka-specific aspects of your application and ensure it is working as expected on the local machine. The cool thing about Dev Services is that it can also start a schema registry (such as ), relational databases, caches, and many other 3rd party service dependencies. Once you are done with the “inner-loop” development step, you want to commit your application to a source control system and run some integration tests on the central build system. You can use to start a Kafka broker from a Java DSL (or for C), and allow you to pick specific Kafka distributions and versions. If your application passes all the gates, it is ready for deployment into a shared environment with other services where a continuously running Kafka cluster is needed. In this post, we are focusing only on the Kafka broker distributions and not the complete Kafka ecosystem of tools and additional components. There are other monitoring and management tools, and services that help developers and operations teams with their daily activities which we leave for another post. Self-managed Kafka Since our application has not reached production or a performance testing environment that requires production-like characteristics, all we want is to have a Kafka installation that is reliable enough for various teams to integrate and run some tests without involving a lot of effort to manage. Another characteristic of such an environment is to be low cost without the cost overhead of data replication and multi-AZ deployment. Many organizations have Kubernetes environments where each development team has their isolated namespace and shared namespaces for CI/CD purposes with all the shared dependencies deployed. project - origicnally created by Red Hat has everything needed to automate and operate a Kafka cluster on Kubernetes for development and production purposes. The advantage of using Strimzi for the lower environments is that it can be managed through a declarative Kubernetes API which is used by developers to manage the applications they develop and other 3’rd party dependencies. This allows developers to use the same Kubernetes infrastructure to quickly create a Kafka cluster for individual or team uses, a temporary project cluster, or a longer living shared cluster, repeatedly through automation pipelines and processes w/o going to depend on other teams for approval and provisioning of new services. List of Apache Kafka distributions and use cases Self-managed Kafka clusters are not used only for development purposes, but for production too. Once you get closer to a production environment, the characteristics required from the Kafka deployment change drastically. You want to be able to provision production-like Kafka clusters for application performance testing and DR testing scenarios. A production environment is not usually a single Kafka cluster either, it can be optimized for different purposes. You may want a self-managed Kafka cluster to deploy on your edge clusters that run offline, on-premise infrastructure that may require a non-standard topology or public cloud with a fairly standard multi-AZ deployment. And there are many self-managed Kafka platforms from Red Hat, Confluent, Cloudera, TIBCO, to name a few. The main characteristic of a self-managed cluster is the fact that the responsibility to manage and run the Kafka cluster resides within the organization owning the cluster. With that, a self-managed cluster also allows customization and configuration of the Kafka cluster, bespoke tuning to suit your deployment needs. For these and any other odd use cases that are not possible with model, the self-managed Kafka remains a proven path. KAFKA AS A SERVICE Each organization is different, but some of the common criteria for production Kafka clusters are things such as the ability to deploy on multiple AZs, on-demand scaling, compliance and certifications, a predictable cost model, open for 3rd party tools and services integrations, and so forth. Today, Kafka is over a decade old and there are multiple mature Kafka as a Service offerings able to satisfy many production needs. While these offerings vary in sizing options, the richness of the user interface, Kafka ecosystem components, and so forth, a key difference is whether Kafka is treated as an infrastructure component or treated as its own event-streaming category with its event-streaming abstractions. Apache Kafka distributions landscape Based on the abstraction criteria we can see that some SaaS providers (such as AWS MSK, Heroku, Instaclustr, Aiven) focus on infrastructure details such as VM sizes, the number of cores and memory, storage types, broker, Zookeeper, Kafka Connect details, and so forth. Many critical decisions about the infra choice, capacity matching to Kafka, Kafka cluster configurations, Zookeeper topology, are left for the user to decide. These services resemble infrastructure services that happen to run Kafka on top, and that is reflected in the VM-based sizing and pricing models. These services have a larger selection of sizing options and can be preferred by teams that have infrastructure inclination and preference to know and control all aspects of a service (even a managed service). Other Kafka as a Service providers (such as Confluent Cloud, AWS MSK Serverless, Red Hat Openshift Streams for Apache Kafka, Upstash) go the opposite “Kafka-first” direction where the infra, the management layer (typically Kubernetes based), and Kafka cluster details are taken care of, and hidden. With these services, the user is dealing with higher level, Kafka-focused abstractions such as Streaming/Kafka/Topic-like units of measure (which represents normalized multi-dimensional Kafka capacity) rather than infrastructure capacity; availability guarantees instead of deployment topology of brokers and Zookeeper; connectors to external systems as an API (regardless of the implementation technology) instead of Kafka Connect cluster deployment and connector deployments. This approach exposes what is important for a Kafka user and not the underlying infrastructure or implementation choices that make up a Kafka service. In addition, these Kafka-first services offer a consumption based Kafka-centric pricing model where the user pays for Kafka capacity used and quality of service rather than provisioned infrastructure with the additional Kafka margin. These services are more suitable for lines of business teams that focus on their business domain and treat Kafka as a commodity tool to solve the business challenges. Next, we will see why Kafka-first managed services are blurring the line and getting closer to a serverless-like Kafka experience where the user is interacting with Kafka APIs and everything else is taken care of. “SERVERLESS-LIKE” KAFKA Serverless technologies are a class of SaaS that have specific characteristics offering additional benefits to users such as a pay-per-use pricing model and eliminating the need for capacity management and scaling altogether. This is achieved through things such as not having to provision and manage servers, built-in high availability, built-in rebalancing, automatic scaling up, and scaling down to zero. We can look at the “Serverless Kafka” category from two opposing angles. On the positive side, we can say that the “Kafka-first” services are getting pretty close to a serverless user experience except for the pricing aspect. With these Kafka-first services, users don't have to provision infrastructure, the Kafka clusters are already preconfigured for high availability, with partition rebalancing, storage expansion, and auto-scaling (within certain boundaries). On the negative side, whether a Kafka service is called serverless or not, these offerings still have significant technical and pricing limitations and they are not mature enough. These services are constrained in terms of message size, partition count, partition limit, network limit, storage limit. These constraints limit the use cases where a so-called serverless Kafka can be used. Other than Upstash which is charging per message, the remaining serverless Kafka services charge for cluster hours which is against the scale-to-zero/pay-per-use ethos of the serverless definition. That is why today I consider the serverless Kafka category still an inspiration rather than reality. Nevertheless, these trends set the direction where managed Kafka offerings are headed: that is complete infrastructure and deployment abstractions hidden from the user; Kafka-first primitives for capacity, usage, quality of a service; autonomous service lifecycle that doesn’t require any user intervention; and with a true pay-for-what-you-use pricing model. SUMMARY How many types of Kafka do you need? The answer is more than one. You want developer frameworks that can emulate and enable rapid, iterative development. You want a declarative and automated way to repeatedly deploy and update development environments. Depending on your business requirements, you may require highly customised Kafka implementations at the edge or standard implementations across multiple clouds that are all connected. While your organization's event streaming adoption and Kafka maturity grows, you will need more Kafka. But there is a paradox. If you are not in the Kafka business, you should work less on Kafka itself and use Kafka for more tasks that set your business apart. This is possible if you use Kafka through higher-level frameworks like Strimzi that automate many of the operational aspects, or through a that takes care of low-level decision-making and relieves you of the responsibility of running Kafka. This way, your teams stop thinking about Kafka and start thinking about how to use Kafka for what matters to your customers. Follow me at to join my journey of learning Apache Kafka. This post was originally published on Red Hat Developers. To read the original post, check and .</content><dc:creator>Unknown</dc:creator></entry><entry><title>Thousands of PyPI and RubyGems RPMs now available for RHEL 9</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9" /><author><name>Miroslav Suchý</name></author><id>28c0e94c-2e99-4c92-ba50-262890b329f8</id><updated>2022-06-07T07:00:00Z</updated><published>2022-06-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9 now offers convenient (but unsupported) access to RPMs from two of the largest and most popular code repositories: The &lt;a href="https://pypi.org"&gt;Python Package Index&lt;/a&gt; (PyPI) for &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; and the &lt;a href="https://rubygems.org"&gt;RubyGems&lt;/a&gt; collection for &lt;a href="https://developers.redhat.com/topics/ruby/all"&gt;Ruby&lt;/a&gt;. This new offering makes it easier to use thousands of community libraries in your projects. We'll look at the repositories in this article.&lt;/p&gt; &lt;h2&gt;The Red Hat repository ecosystem and COPR&lt;/h2&gt; &lt;p&gt;Red Hat supports about 2,500 packages. They are of very high quality, created and maintained by experts. Outside our supported offerings for RHEL 9, we also have &lt;a href="https://www.redhat.com/en/blog/whats-epel-and-how-do-i-use-it"&gt;Extra Packages for Enterprise Linux&lt;/a&gt; (EPEL 9) with an additional 3,000 packages. But there are a lot of other important libraries and utilities on the internet.&lt;/p&gt; &lt;p&gt;We certainly cannot package everything. But we have decided to provide you with an additional 151,000 RPM packages by packaging everything we can (with an exception we'll discuss in a moment) in PyPI and RubyGems. The packages have been added to another set of unsupported packages called &lt;a href="https://copr.fedorainfracloud.org/"&gt;COPR&lt;/a&gt;, which stands for &lt;strong&gt;co&lt;/strong&gt;mmunity &lt;strong&gt;pr&lt;/strong&gt;ojects. Both EPEL and COPR were formed by the &lt;a href="https://fedoraproject.org/"&gt;Fedora project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Benefits of using the signed Red Hat repositories&lt;/h2&gt; &lt;p&gt;As noted, Red Hat does not offer support for COPR packages. Each is provided as-is, without warranty. If you have an issue with some package, please contact an upstream author.&lt;/p&gt; &lt;p&gt;However, Red Hat does provide some security through signatures. Packages on PyPI &lt;a href="https://github.com/pypa/pip/issues/425"&gt;are not signed&lt;/a&gt;, but Red Hat signs the RPM packages in COPR repositories. This means that you can audit the packages: You know which files belong to which package and vice versa, and you will be able to check whether a file was altered by a user.&lt;/p&gt; &lt;p&gt;To enable the PyPI COPR on your computer, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# dnf copr enable @copr/PyPI epel-9-x86_64&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, if &lt;code&gt;pip install foo&lt;/code&gt; works for you, installing &lt;code&gt;python3-foo&lt;/code&gt; from this repository will work too.&lt;/p&gt; &lt;p&gt;&lt;span&gt; &lt;/span&gt;To enable the RubyGems COPR, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# dnf copr enable @rubygems/rubygems epel-9-x86_64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having enabled the COPRs on your system, you can get the signed keys by entering:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# dnf install distribution-gpg-keys # rpm --import /usr/share/distribution-gpg-keys/copr/copr-@copr-PyPI.gpg # rpm --import /usr/share/distribution-gpg-keys/copr/copr-@rubygems-rubygems.gpg&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Details and limitations&lt;/h2&gt; &lt;p&gt;Not all packages from PyPI and RubyGems could be included in our COPRs. Some packages suffered from build problems, which I'll describe later. Others were excluded due to licensing conflicts. If the maintainers of a package fail to explicitly assign a free or &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; license, they leave the gem by default under conventional "All rights reserved" copyright. Because these packages are not open source, we cannot distribute them.&lt;/p&gt; &lt;p&gt;We provide the latest version of each Python package and RubyGem. When a new version is published, we rebuild it and update the repository. The previous version is deleted after 14 days. If you need an older version, upload it to &lt;a href="https://access.redhat.com/products/red-hat-satellite"&gt;Red Hat Satellite&lt;/a&gt; or keep a local copy.&lt;/p&gt; &lt;h2&gt;What we built from PyPI&lt;/h2&gt; &lt;p&gt;Many improvements to Python packaging were made in recent years, both upstream and downstream in RPM.&lt;/p&gt; &lt;p&gt;With new RPM macros created by Python maintainers in Red Hat, it is now possible to create a deterministic converter from Python package metadata to RPM specfiles. We used a new tool, &lt;a href="https://github.com/befeleme/pyp2spec/"&gt;pyp2spec&lt;/a&gt;, making use of those new RPM macros, to rebuild PyPI packages as RPMs in COPR.&lt;/p&gt; &lt;p&gt;Starting in December 2021, we tried to rebuild all packages on PyPI (more than 330,000 at the time) in Fedora Rawhide. When it became possible to build packages for EPEL 9 in COPR, we went ahead and reran the build. We completed this project, building 79,842 packages for Red Hat Enterprise Linux 9 and making them available in the &lt;a href="https://copr.fedorainfracloud.org/coprs/g/copr/PyPI/"&gt;PyPI COPR&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Only the packages that were built successfully for Fedora Rawhide were submitted for the Red Hat Enterprise Linux 9 rebuild. Trying to rebuild the whole PyPI repository would require months, and there was little chance that any given EPEL build would succeed if the Rawhide one didn't.&lt;/p&gt; &lt;p&gt;Based on the build logs of the failed packages, we found the following problems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;32% had missing build dependencies in our COPR repository, often because a package required a specific version of a dependency.&lt;/li&gt; &lt;li&gt;20% didn't have a license or specified a license that wasn't detected as open source.&lt;/li&gt; &lt;li&gt;About 13% of packages had an error somewhere in the upstream configuration leading to the build failure. Typically, the problem was due either to missing files in the source archive or to failures to import modules that weren't declared as build dependencies.&lt;/li&gt; &lt;li&gt;12% of packages didn't have the source archive uploaded to PyPI, which prevented us from building the RPM. If a package was built successfully for Rawhide but not for EPEL, the typical culprit was missing dependencies.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you encounter a package that's installable from &lt;code&gt;pip&lt;/code&gt;, but is not available in our COPR, you can try contacting the upstream maintainers to discuss loosening version requirements for dependencies, fixing the license declaration, uploading the source archive to PyPI, or resolving other problems as the case may be.&lt;/p&gt; &lt;p&gt;For instructions on how to reproduce the build, see our &lt;a href="https://copr.fedorainfracloud.org/coprs/g/copr/PyPI/"&gt;PyPI COPR&lt;/a&gt; repository.&lt;/p&gt; &lt;h2&gt;What we built from RubyGems&lt;/h2&gt; &lt;p&gt;Support for building RPM packages in COPR directly from RubyGems.org was &lt;a href="http://frostyx.cz/posts/copr-rubygems"&gt;introduced back in 2016&lt;/a&gt;. The procedure uses a tool called &lt;a href="https://github.com/fedora-ruby/gem2rpm"&gt;gem2rpm&lt;/a&gt; to convert gem metadata into an RPM specfile and produce an SRPM package based on it.&lt;/p&gt; &lt;p&gt;Utilizing this feature, we have rebuilt all of RubyGems.org for Fedora Rawhide. In &lt;a href="http://frostyx.cz/posts/rebuilding-the-entire-rubygems-in-copr"&gt;this detailed blog post&lt;/a&gt;, you can find more information about the success rate, the size of the packages and their metadata, COPR internals, and takeaways for the RPM toolchain.&lt;/p&gt; &lt;p&gt;Countless performance improvements, and months of building later, we are now announcing that all of RubyGems.org was rebuilt for Red Hat Enterprise Linux 9. The &lt;a href="https://copr.fedorainfracloud.org/coprs/g/rubygems/rubygems"&gt;RubyGems project in COPR&lt;/a&gt; provides 71,952 packages, which is almost half of the RubyGems.org service. We got 19,635 failures because of unmet dependencies, and around 77,000 gems were skipped and not even attempted to be built because of missing licenses. A full 37% of gems in RubyGems do not specify a &lt;a href="https://guides.rubygems.org/specification-reference/#license="&gt;license&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If your gem isn't available from the RubyGems COPR, it is most likely due to a missing license. Please resolve such issues with the respective gem owners. The same applies to missing dependencies. If you find a problem with the generated specfiles, please file &lt;a href="https://github.com/fedora-ruby/gem2rpm/issues"&gt;a new issue&lt;/a&gt; for &lt;code&gt;gem2rpm&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our PyPI and RubyGems projects demonstrate Red Hat's desire to help programmers make the most of free and open source resources. During our months-long efforts, we turned up weaknesses in the source repositories that guide upstream developers to produce more robust packages.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9" title="Thousands of PyPI and RubyGems RPMs now available for RHEL 9"&gt;Thousands of PyPI and RubyGems RPMs now available for RHEL 9&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Miroslav Suchý</dc:creator><dc:date>2022-06-07T07:00:00Z</dc:date></entry><entry><title type="html">Creating tabs using DashBuilder</title><link rel="alternate" href="https://blog.kie.org/2022/06/creating-tabs-using-dashbuilder%ef%bf%bc.html" /><author><name>Nikhil Dewoolkar</name></author><id>https://blog.kie.org/2022/06/creating-tabs-using-dashbuilder%ef%bf%bc.html</id><updated>2022-06-06T16:47:50Z</updated><content type="html">DashBuilder is a full-featured web application that allows non-technical users and programmers to create business dashboards. Dashboard data can be extracted from heterogeneous sources of information such as Prometheus, JDBC databases, or regular text files. The resulting dashboard can be deployed in a cloud environment using DashBuilder Runtime. In this post, we will walk you through creating tabs using DashBuilder to facilitate  better organization of your dashboards. Home Page of dashbuilder Adding dataset Step 1:- To create tablists, we should add a dataset to DashBuilder. Click on the datasets option and then click on the new dataset. You will get options for dataset types like Bean, CSV, Json, SQL,Prometheus, Kafka, External and Execution server. For this blog post, we will use CSV as an option. Dataset List Page Step 2: Select dataset type(we are using CSV as dataset for this example).Going forward to the next step, you will land on the following screen. Enter name for dataset, then select CSV file and then click on upload button which is beside the file select button. Enter the correct separation char(in this example comma (,) is separation char) and quote char(in this example comma (“) is quote char) and then click on the test button. Dataset link:- Adding CSV Dataset Step 3:- You will now get the following screen. If you want to remove some columns then unselect the columns else click on the Next button.  Enter a comment and click on the Save button. Finalizing columns in datasets Creating New Pages and group for tablists:- Step 1:- Creating new pages procedure is available in the following post:- Create 4-5 pages with some design in it so that we can create a tablist using the navigation page. After creating pages you will see following screen Create Pages to be added to tablist Step 2:- Go to the navigation option icon which is just below the pages icon.     Navigation Page Step3:- Click on the setting icon in front of dashboards and click on the new group. And give a group name to it.  Now, click on the setting icon again in front of your created group. Add pages which you have created earlier using the pages icon and select pages from the dropdown list.You can now add all the pages to the group. Creating new group Creating Tablist:- Step 1:- Go to pages option and create one page to add tablist to it. Then drag and drop target div to pages. Give it a name. Creating new page to add tablist Adding target div to page Step 2:- Under the navigation option drag and drop the Tablist to page. Select the proper group option then give the default page name and select the target div you created. Click on Ok . Your tablist will be created. The following pictures will explain this step fully. Adding tablist to Page  Creating new tablist     Tablist added Conclusion:- This blogpost explains all the steps to create a tablist in DashBuilder.  Don’t forget to check out the newest addition to DashBuilder.i.e, the YML editor which doesn’t require any extra installation on your end. Creating a dashboard using pure YML from data from any JSON document is now possible using Dashbuilder! You are just required to head over to the and start your dashboard! You can check out the sample dashboards built using YML editor on the . If you want to create your own dashboards, feel free to refer to the . Feel free to explore all and let us know in the comments section if this post was of some help. The post appeared first on .</content><dc:creator>Nikhil Dewoolkar</dc:creator></entry><entry><title>What's new in version 2.7 of the Red Hat build of Quarkus</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/06/whats-new-version-27-red-hat-build-quarkus" /><author><name>Jeff Beck</name></author><id>2d202f19-5acc-4180-9b53-143943e3e72e</id><updated>2022-06-06T07:00:00Z</updated><published>2022-06-06T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat recently released version 2.6 of the Red Hat build of Quarkus to support enterprise developers building &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;-native &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; applications. The latest release has several great new features and performance improvements, including tools to improve developer productivity.&lt;/p&gt; &lt;p&gt;Let’s take a look at some highlights from this release. For a complete list, check out the &lt;a href="http://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus/quarkus-2-7/guide/5cbab82b-042a-4a68-ac5e-54901a9cc222"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Developer productivity&lt;/h2&gt; &lt;p&gt;One of the founding principles of Quarkus is to bring joy to Java developers by creating a frictionless experience through a combination of tools, libraries, extensions, and more. This release continues that mission by making developers more efficient with new tools and services such as Hibernate search, dev services, and a Kafka UI.&lt;/p&gt; &lt;h3&gt;Hibernate search&lt;/h3&gt; &lt;p&gt;Hibernate search is now fully supported, providing a powerful tool for free text search capability to data. Normal Jakarta Persistence API (JPA) queries are limited to SQL queries which only query per column. With Hibernate search, developers can easily express how their Java model should be indexed and searchable. Hibernate search can also extend web page search to a full-text search of your Java domain, including searches for synonyms, sound-alike words, and more. Check out the &lt;a href="https://quarkus.io/guides/hibernate-search-orm-elasticsearch"&gt;documentation&lt;/a&gt; and the &lt;a href="https://www.youtube.com/watch?v=hwxWx-ORVwM"&gt;Quarkus Insights session&lt;/a&gt; to learn more, or watch this video on Hibernate Search from Red Hat Developer.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h3&gt;Dev services&lt;/h3&gt; &lt;p&gt;When an application starts in dev mode, dev services provide automatic provisioning of unconfigured services, such as a database or identity server. When Quarkus adds the service extension, it automatically starts the relevant service and wires your application to use it. This release adds to the growing list of dev services, including &lt;a href="https://quarkus.io/guides/dev-services#neo4j"&gt;Neo4j&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/dev-services#keycloak"&gt;Keycloak&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/dev-services#infinispan"&gt;Infinispan&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/dev-services#vault"&gt;Vault&lt;/a&gt;, and &lt;a href="https://quarkus.io/guides/dev-services"&gt;more&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Kafka UI&lt;/h3&gt; &lt;p&gt;The Kafka UI (Figures 1 and 2) allows developers to visualize Apache Kafka Streams for reactive applications within the &lt;a href="https://quarkus.io/guides/dev-ui"&gt;Dev UI&lt;/a&gt;. The UI shows how the event streams sink in topics and how applications consume the streams from topics. The UI illustrates how the application aggregates streams from multiple topics to a single topic. The UI also showcases the continuous sequence of sourcing, joining, and aggregating streams in Kafka clusters. The Red Hat Developer article &lt;a href="https://developers.redhat.com/articles/2021/12/07/visualize-your-apache-kafka-streams-using-quarkus-dev-ui"&gt;Visualize your Apache Kafka Streams using the Quarkus Dev UI&lt;/a&gt; has more on this topic.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dev_1.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dev_1.jpg?itok=99ED6Ycu" width="600" height="326" alt="Quarkus Dev UI with Kafka Streams" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Dev UI Console &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Dev UI console.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic_0_0.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topic_0_0.jpg?itok=M42ix0hv" width="600" height="403" alt="Quarkus Kafka UI" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Quarkus Kafka UI &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Quarkus Kafka UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Performance and framework efficiency&lt;/h2&gt; &lt;p&gt;Quarkus makes Java supersonic and subatomic, with fast startup times, low memory footprint, and a small disk footprint. This release continues to improve the Quarkus framework's performance and efficiency.&lt;/p&gt; &lt;h3&gt;UPX&lt;/h3&gt; &lt;p&gt;UPX is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt;, portable, high-performance executable packer that takes an executable as input and produces a compressed executable. Quarkus can now automatically perform UPX compression as part of the &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; creation process. By using UPX to compress a native executable, developers can build containers that are smaller than 30MB with Quarkus in them. &lt;a href="https://quarkus.io/blog/upx/"&gt;Learn more&lt;/a&gt; about compressing native executables using UPX.&lt;/p&gt; &lt;h2&gt;Kubernetes-Native&lt;/h2&gt; &lt;p&gt;Quarkus enables Java developers to create performant, easily deployable and maintainable applications on Kubernetes and the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. This release includes many impactful Kubernetes-Native features.&lt;/p&gt; &lt;h3&gt;SmallRye Stork&lt;/h3&gt; &lt;p&gt;In a distributed &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice architecture&lt;/a&gt;, discovery and load balancing can be challenging with interactions between services. Developers can use Stork (currently in tech preview) to fully abstract away the service discovery for Consul, Eureka, Kubernetes/OpenShift, or static machines. Learn more about how to use SmallRye Stork in the &lt;a href="https://quarkus.io/guides/stork"&gt;Quarkus guide&lt;/a&gt;, or check out this Red Hat Developer video on the subject.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Community and standards&lt;/h2&gt; &lt;p&gt;Quarkus starts and ends with the community. Quarkus includes a vast ecosystem of over 400 extensions and support for popular Java APIs and standards. This release continues that mission with new features and standards support.&lt;/p&gt; &lt;h3&gt;Java 17 support&lt;/h3&gt; &lt;p&gt;This release includes improved developer tools for using Java 17. Now you can &lt;a href="https://code.quarkus.redhat.com/"&gt;generate projects&lt;/a&gt; based on Java 17 without manually configuring the version. This release also includes a Technology Preview for Java 17 in native mode (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Untitled%20presentation%20%287%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Untitled%20presentation%20%287%29.png?itok=LAd5kUss" width="600" height="338" alt="Quarkus code generator for Java 17" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Quarkus code generator for Java 17 &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Quarkus code generator for Java 17.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Try Quarkus&lt;/h2&gt; &lt;p&gt;The best way to experience the new features of Quarkus is to start using it. Generate your code at &lt;a href="http://code.quarkus.redhat.com"&gt;code.quarkus.redhat.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can find more Quarkus resources on Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/e-books/quarkus-spring-developers"&gt;Quarkus for Spring Developers&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/books/quarkus-cookbook"&gt;Quarkus Cookbook&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Learn Quarkus faster with quick starts in the &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/06/whats-new-version-27-red-hat-build-quarkus" title="What's new in version 2.7 of the Red Hat build of Quarkus"&gt;What's new in version 2.7 of the Red Hat build of Quarkus&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jeff Beck</dc:creator><dc:date>2022-06-06T07:00:00Z</dc:date></entry><entry><title type="html">Creating Quarkus applications using IntelliJ IDEA</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/creating-quarkus-projects-using-intellij-idea/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/creating-quarkus-projects-using-intellij-idea/</id><updated>2022-06-06T05:22:00Z</updated><content type="html">This tutorial introduces you to JetBrains IntelliJ Quarkus plugin which lets you bootstrap and develop Quarkus projects in the simplest and most productive way. Today, there is a wealth of plugins available to develop applications with Quarkus (including Visual Studio, Eclipse, Eclipse Che and IntelliJ Idea). This article focuses on IntelliJ Plugin for Quarkus which ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Vlog: Interview with Farah Juma</title><link rel="alternate" href="https://www.youtube.com/watch?v=3qT5wf57vDg" /><author><name>Stefano Maestri</name></author><id>https://www.youtube.com/watch?v=3qT5wf57vDg</id><updated>2022-06-06T00:00:00Z</updated><dc:creator>Stefano Maestri</dc:creator></entry><entry><title type="html">Solving Timeout waiting for service container stability</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-deploy/how-to-solve-timeout-for-service-container-stability/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-deploy/how-to-solve-timeout-for-service-container-stability/</id><updated>2022-06-02T09:54:16Z</updated><content type="html">In some scenarios, typically container applications, it is quite common to hit the error “WFLYCTL0348: Timeout after [300] seconds waiting for service container stability“. This article will guide through the steps to find the root cause and solve the issue. What is container stability? Firstly, let’s start from the following message which you have found ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to create Kafka consumers and producers in Java</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/02/how-create-kafka-consumers-and-producers-java" /><author><name>Bob Reselman</name></author><id>008ad361-460e-41fc-92bb-49e375d1d57b</id><updated>2022-06-02T07:00:00Z</updated><published>2022-06-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt; has emerged as one of the more popular open source technologies for powering message-driven applications at web scale. It can handle hundreds of thousands, if not millions of messages a second. And, it will store any message it receives for a configurable amount of time, whether the message is consumed or not. That timespan can be weeks, or it can be years. These capabilities alone make Kafka a very powerful technology.&lt;br /&gt; &lt;br /&gt; Yet, while Kafka is powerful, it is also complex. There are many moving parts that need to be accounted for. Fortunately, there are client libraries written in a variety of languages that abstract away much of the complexity that comes with programming Kafka. One of the more popular libraries is the one written for &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt;, which is published as the &lt;a href="https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients"&gt;Apache Kafka Clients&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Java client libraries for Kafka streamline a lot of the work that goes into producing messages to and consuming messages from Kafka brokers. But they are not magical. You need to know a thing or two in order to use them effectively. Java is an object-oriented programming (OOP) language, which means there's a bit more to understand, particularly if you are new to OOP.&lt;/p&gt; &lt;p&gt;This article aims to provide an introductory helping hand to get you started working with Java producers and consumers from an object-oriented perspective. It covers the basics of creating and using Kafka producers and consumers in Java. You'll learn how to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Work with a Java properties file to configure programmatic access to Kafka brokers.&lt;/li&gt; &lt;li&gt;Program a Kafka producer in Java to emit messages to a broker.&lt;/li&gt; &lt;li&gt;Program a Kafka consumer in Java to retrieve messages from a broker.&lt;/li&gt; &lt;li&gt;Install Kafka as a &lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman&lt;/a&gt; or Docker &lt;a href="https://developers.redhat.com/topic/containers"&gt;container&lt;/a&gt; for use by both producers and consumers.&lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is the second article in our series about working with Kafka and Java. If you're new to Kafka and Java concepts, be sure to read the previous installment, &lt;a href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1"&gt;A developer's guide to using Kafka with Java&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;&lt;span&gt;This article ships with a &lt;a href="https://github.com/redhat-developer-demos/kafka-java-demo.git"&gt;demonstration application&lt;/a&gt; that is the focus of this article. You can run this application's code from any &lt;/span&gt;&lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;&lt;span&gt; computer with Java and Maven installed. The code is painstakingly commented in order to provide a greater understanding of the programming details. Each line of code has a purpose. Understanding that purpose is important.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;I do assume that you understand the basics of OOP, that you know what &lt;a href="https://docs.oracle.com/javase/tutorial/java/concepts/package.html"&gt;packages&lt;/a&gt;, &lt;a href="https://docs.oracle.com/javase/tutorial/java/concepts/class.html"&gt;classes&lt;/a&gt;, &lt;a href="https://docs.oracle.com/javase/tutorial/java/concepts/interface.html"&gt;interfaces&lt;/a&gt;, and &lt;a href="https://www.techopedia.com/definition/17408/abstract-class"&gt;abstract classes&lt;/a&gt; are, and that you understand what class inheritance is about. I also assume that you understand the fundamentals of variable scope, and that you know what the keywords &lt;em&gt;private, protected,&lt;/em&gt; and &lt;em&gt;public&lt;/em&gt; mean.&lt;/p&gt; &lt;p&gt;The illustrations in this article generally follow the guidelines defined in Unified Modeling Language (&lt;a href="https://www.uml.org/"&gt;UML&lt;/a&gt;). To follow along, it's not essential to understand the fine points of UML—understanding the meaning of the arrowhead symbols used with connector lines, for example—but it won't hurt if you do.&lt;/p&gt; &lt;p&gt;Finally, the demonstration code for this article is a Maven project, so having a basic idea of what &lt;a href="https://maven.apache.org/"&gt;Maven&lt;/a&gt; is about is useful, but not required.&lt;/p&gt; &lt;h2&gt;The demonstration project object model&lt;/h2&gt; &lt;p&gt;You can download the article's demonstration project from the Red Hat Developer GitHub repository by executing the following command from a Terminal window:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/redhat-developer-demos/kafka-java-demo.git&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The demonstration project is written in Java and uses the Maven application management framework. Thus, both the Java Development Kit and Maven need to be installed on the Linux computer you plan to use, should you decide to follow along in code. (The &lt;code&gt;readme.md&lt;/code&gt; file in the demonstration project source code provides instructions for this.)&lt;/p&gt; &lt;p&gt;The demonstration project publishes an &lt;code&gt;Application&lt;/code&gt; class that is the starting point of execution. This class has a &lt;code&gt;main()&lt;/code&gt; method that starts either a Kafka producer or a Kafka consumer. Which one it starts will depend on the values passed as parameters to &lt;code&gt;main()&lt;/code&gt; at the command line. For example, to have the &lt;code&gt;Application&lt;/code&gt; class start a Kafka producer, you'd type the following in a terminal window from the root of the working directory of the demonstration application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn -q clean compile exec:java \ -Dexec.mainClass="com.demo.kafka.Application" \ -Dexec.args="producer mytopic" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The class that encapsulates the Kafka producer is named &lt;code&gt;SimpleProducer&lt;/code&gt;. The class that encapsulates the Kafka consumer is named &lt;code&gt;SimpleConsumer&lt;/code&gt;. As the names imply, &lt;code&gt;SimpleProducer&lt;/code&gt; emits messages to the Kafka broker, and those messages are retrieved and processed by &lt;code&gt;SimpleConsumer&lt;/code&gt;. (In this sample application, these messages just contain random text data.) All this is illustrated in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-01.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-01.png?itok=E8BxNtPF" width="600" height="65" alt="Diagram showing an application emitting messages with random data to a Kafka broker and consuming those messages" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The demonstration application can emit messages with random data to a Kafka broker or consume those messages.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Both &lt;code&gt;SimpleProducer&lt;/code&gt; and &lt;code&gt;SimpleConsumer&lt;/code&gt; get their configuration values from a helper class named &lt;code&gt;PropertiesHelper&lt;/code&gt;. &lt;code&gt;PropertiesHelper&lt;/code&gt; uses the values declared in a text file named &lt;code&gt;config.properties&lt;/code&gt; to create a &lt;a href="https://docs.oracle.com/javase/7/docs/api/java/util/Properties.html"&gt;Java Properties&lt;/a&gt; class. This &lt;code&gt;Properties&lt;/code&gt; class exposes all the values in &lt;code&gt;config.properties&lt;/code&gt; as key-value pairs, as illustrated in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-02.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-02.png?itok=YTO81t76" width="600" height="266" alt="Diagram showing that the Application class in the demonstration project invokes either a Kafka producer or Kafka consumer" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Application class in the demonstration project invokes either a Kafka producer or Kafka consumer.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The code's configuration settings are encapsulated into a helper class to avoid violating the DRY (or &lt;em&gt;Don't Repeat Yourself&lt;/em&gt;) principle. The &lt;code&gt;config.properties&lt;/code&gt; file is the single source of truth for configuration information for both the producer and consumer classes. You could just create and populate a &lt;code&gt;Properties&lt;/code&gt; object directly in both the producer and consumer code in order to provide runtime information, but having consumers and producers use the &lt;code&gt;PropertiesHelper&lt;/code&gt; class to get runtime information means that, if a configuration value needs to be added or changed, all the developer needs to do is to alter the contents of the &lt;code&gt;config.properties&lt;/code&gt; file and then recompile. There is no need to fiddle around with actual Java code.&lt;/p&gt; &lt;h2&gt;Working with the KafkaClient properties&lt;/h2&gt; &lt;p&gt;As earlier noted, the &lt;code&gt;config.properties&lt;/code&gt; file is the single source of truth for defining runtime information that the consumers and producers need to bind to a Kafka broker. The following listing shows the contents of this file for the demonstration application, commented to explain the purpose of each entry.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# The location of the Kafka server bootstrap.servers=localhost:9092 # the default group ID group.id=test-group # the default topic to use if one is not provided default.topic=magic-topic # The number of records to pull of the stream every time # the client takes a trip out to Kafka max.poll.records=10 # Make Kafka keep track of record reads by the consumer enable.auto.commit=true # The time in milliseconds to Kafka write the offset of the last message read auto.commit.interval.ms=500 # classes for serializing ... key.serializer=org.apache.kafka.common.serialization.StringSerializer value.serializer=org.apache.kafka.common.serialization.StringSerializer # ... and deserializing messages key.deserializer=org.apache.kafka.common.serialization.StringDeserializer value.deserializer=org.apache.kafka.common.serialization.StringDeserializer&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now that you understand how runtime information stored in a properties file is exposed by way of a helper class, you're ready to learn how the &lt;code&gt;SimpleProducer&lt;/code&gt; class emits messages to a Kafka broker.&lt;/p&gt; &lt;h2&gt;The Kafka producer&lt;/h2&gt; &lt;p&gt;&lt;code&gt;SimpleProducer&lt;/code&gt; has the code that first creates a message that contains a random string and then sends that message on to the Kafka broker. The location of the Kafka broker is defined in the &lt;code&gt;config.properties&lt;/code&gt; file. The actual class that does the work of sending the message to the Kafka broker is called &lt;code&gt;KafkaProducer&lt;/code&gt;, which is part of the Apache Kafka client package. It has all the intelligence required to work with a Kafka broker.&lt;/p&gt; &lt;p&gt;&lt;code&gt;SimpleProducer&lt;/code&gt; is inherited from the abstract class named &lt;code&gt;AbstractSimpleKafka&lt;/code&gt;. &lt;code&gt;AbstractSimpleKafka&lt;/code&gt; provides the graceful shutdown behavior for &lt;code&gt;SimpleProducer&lt;/code&gt;. That shutdown behavior is also used by the &lt;code&gt;SimpleConsumer&lt;/code&gt; class that you'll learn more about shortly.&lt;/p&gt; &lt;p&gt;&lt;code&gt;AbstractSimpleKafka&lt;/code&gt; declares two abstract methods named &lt;code&gt;shutdown()&lt;/code&gt; and &lt;code&gt;runAlways()&lt;/code&gt;. Declaring these methods as abstract means that a class that inherits from &lt;code&gt;AbstractSimpleKafka&lt;/code&gt; must provide implementation logic for them. This is appropriate because the classes that inherit from &lt;code&gt;AbstractSimpleKafka&lt;/code&gt; will have a particular way of implementing their behavior. Declaring &lt;code&gt;shutdown()&lt;/code&gt; and &lt;code&gt;runAlways()&lt;/code&gt; as abstract ensures that they will be implemented.&lt;/p&gt; &lt;p&gt;The demonstration application uses an abstract class here in order to support the DRY principle. Since both &lt;code&gt;KafkaProducer&lt;/code&gt; and &lt;code&gt;SimpleConsumer&lt;/code&gt; have common shutdown behavior, it's best to put the behavior that can be shared in a single location, hence the justification for &lt;code&gt;AbstractSimpleKafka&lt;/code&gt;. Logic in &lt;code&gt;AbstractSimpleKafka&lt;/code&gt; does the work of intercepting process termination signals coming from the operating system and then calling the &lt;code&gt;shutdown()&lt;/code&gt; method. All that's required for a class that inherits from &lt;code&gt;AbstractSimpleKafka&lt;/code&gt; is to provide graceful shutdown behavior specific to the inheriting class by overriding the &lt;code&gt;shutdown()&lt;/code&gt; method. All this is illustrated in Figure 3.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-03.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-03.png?itok=bDkWlFpC" width="600" height="228" alt="Diagram showing the he SimpleProducer class emitting messages with random text data to a Kafka broker" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The SimpleProducer class emits messages with random text data to a Kafka broker.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;All that's required to get a new instance of &lt;code&gt;KafkaProducer&lt;/code&gt; that's bound to a Kafka broker is to pass the configuration values defined in &lt;code&gt;config.properties&lt;/code&gt; as a &lt;code&gt;Properties&lt;/code&gt; object to the &lt;code&gt;KafkaProducer&lt;/code&gt; constructor. As mentioned, the &lt;code&gt;PropertiesHelper&lt;/code&gt; class encapsulates loading the keys and values defined in the &lt;code&gt;config.properties&lt;/code&gt; files and then exposing the information as a &lt;code&gt;Properties&lt;/code&gt; object. The following listing shows the getter method that returns the internal &lt;code&gt;KafkaProducer&lt;/code&gt; object that &lt;code&gt;SimpleProducer&lt;/code&gt; uses.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;private KafkaProducer&lt;String, String&gt; getKafkaProducer() throws Exception { if (this.kafkaProducer == null) { Properties props = PropertiesHelper.getProperties(); this.kafkaProducer = new KafkaProducer&lt;&gt;(props); } return this.kafkaProducer; }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;In the next listing, the &lt;code&gt;SimpleProducer&lt;/code&gt; method &lt;code&gt;runAlways()&lt;/code&gt; uses the protected method &lt;code&gt;send()&lt;/code&gt; to emit messages to the Kafka broker defined in &lt;code&gt;config.properties&lt;/code&gt;. The method will send messages continuously every 100 milliseconds until the process in which the class is running is terminated.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; public void runAlways(String topicName, KafkaMessageHandler callback) throws Exception { while (true) { String key = UUID.randomUUID().toString(); //use the Message Helper to get a random string String message = MessageHelper.getRandomString(); //send the message this.send(topicName, key, message); Thread.sleep(100); } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;send()&lt;/code&gt; method uses the values of its key and message parameters passed to create a &lt;code&gt;ProducerRecord&lt;/code&gt; object. This object is the lingua franca by which messages are passed to a Kafka broker by way of the internal &lt;code&gt;KafkaProducer&lt;/code&gt; object.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;protected void send(String topicName, String key, String message) throws Exception { String source = SimpleProducer.class.getName(); //create the ProducerRecord object which will //represent the message to the Kafka broker. ProducerRecord&lt;String, String&gt; producerRecord = new ProducerRecord&lt;&gt;(topicName, key, message); //Use the helper to create an informative log entry in JSON format JSONObject obj = MessageHelper.getMessageLogEntryJSON(source, topicName, key, message); log.info(obj.toJSONString()); //Send the message to the Kafka broker using the internal //KafkaProducer getKafkaProducer().send(producerRecord); }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;send()&lt;/code&gt; method is declared as protected to make it visible for unit testing. Because &lt;code&gt;send()&lt;/code&gt; is only used internally within &lt;code&gt;SimpleProducer&lt;/code&gt;, it's illogical to give the method public visibility. Best practices dictate that you don't want to expose code to the public that doesn't warrant such visibility. But the method does need to be unit tested, and making &lt;code&gt;send()&lt;/code&gt; private would deny a unit test access to it. Setting its visibility to protected makes it visible to any other class in the package &lt;code&gt;com.demo.kafka&lt;/code&gt;. Both &lt;code&gt;SimpleProducer&lt;/code&gt; and the testing class, &lt;code&gt;SimpleProducerConsumerTest&lt;/code&gt;, are part of the same &lt;code&gt;com.demo.kafka package&lt;/code&gt;. Thus, &lt;code&gt;send()&lt;/code&gt; is testable from the testing class &lt;code&gt;SimpleProducerConsumerTest&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Making &lt;code&gt;send()&lt;/code&gt; protected in scope is a subtle point, but it's an important one from the point of view of object-oriented programming.&lt;/p&gt; &lt;p&gt;These are the most important points about the Kafka producer implementation. Next, you'll see how the class &lt;code&gt;SimpleConsumer&lt;/code&gt; retrieves and processes messages from a Kafka broker.&lt;/p&gt; &lt;h2&gt;The Kafka consumer&lt;/h2&gt; &lt;p&gt;&lt;code&gt;SimpleConsumer&lt;/code&gt; is a "wrapper" class that uses an Apache Kafka client's &lt;code&gt;KafkaConsumer&lt;/code&gt; class to retrieve and process messages from a Kafka broker, as illustrated in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-04.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-04.png?itok=rU_cjVRS" width="600" height="129" alt="Diagram showing that messages are retrieved and processed in the demonstration application by the SimpleConsumer class" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Messages are retrieved and processed in the demonstration application by the SimpleConsumer class.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The actual work of continuously getting messages from the Kafka broker and processing them is done by the method &lt;code&gt;runAlways()&lt;/code&gt;. Remember, &lt;code&gt;runAlways()&lt;/code&gt; was declared in the abstract class &lt;code&gt;AbstractSimpleKafka&lt;/code&gt; as an abstract method that was not yet implemented. Since &lt;code&gt;SimpleConsumer&lt;/code&gt; inherits from &lt;code&gt;AbstractSimpleKafka&lt;/code&gt;, it must provide implementation logic for this method.&lt;/p&gt; &lt;p&gt;As you can see in the listing below, the code gets the &lt;code&gt;Properties&lt;/code&gt; object that contains the information to bind to the Kafka broker using the &lt;code&gt;getProperties()&lt;/code&gt; method in the helper class &lt;code&gt;PropertiesHelper&lt;/code&gt;. The properties are passed as a parameter to the constructor of the internal &lt;code&gt;KafkaConsumer&lt;/code&gt; object.&lt;/p&gt; &lt;p&gt;&lt;code&gt;KafkaConsumer&lt;/code&gt; then subscribes to the topic. The name of the topic is passed as a parameter to the &lt;code&gt;runAlways()&lt;/code&gt; method. The &lt;code&gt;KafkaConsumer&lt;/code&gt; object polls the broker at set intervals to get messages. The polling timespan is set according to the value assigned to the class variable &lt;code&gt;TIME_OUT_MS&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; public void runAlways(String topic, KafkaMessageHandler callback) throws Exception { Properties props = PropertiesHelper.getProperties(); kafkaConsumer = new KafkaConsumer&lt;&gt;(props); //keep running forever or until shutdown() is called from another thread. try { kafkaConsumer.subscribe(List.of(topic)); while (!closed.get()) { ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(TIME_OUT_MS)); if (records.count() == 0) { log.info(MessageHelper.getSimpleJSONObject("No records retrieved")); } for (ConsumerRecord&lt;String, String&gt; record : records) { callback.processMessage(topic, record); } } } catch (WakeupException e) { // Ignore exception if closing if (!closed.get()) throw e; } } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Every time a poll is conducted against the Kafka broker, an array of messages comes back as a &lt;code&gt;ConsumerRecords&lt;/code&gt; object. &lt;code&gt;ConsumerRecords&lt;/code&gt; is a collection of &lt;code&gt;ConsumerRecord&lt;/code&gt; objects. The maximum number of messages returned from a poll is defined by the value of the &lt;code&gt;max.poll.records&lt;/code&gt; entry in the &lt;code&gt;config.properties&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;The code uses a callback object to process each message that's retrieved from the &lt;code&gt;Kafka&lt;/code&gt; broker. The callback object is passed as a parameter to the &lt;code&gt;runAlways()&lt;/code&gt; method. As you can see in the listing above, the code traverses the &lt;code&gt;ConsumerRecords&lt;/code&gt; collection retrieved from the poll and invokes the &lt;code&gt;processMessage()&lt;/code&gt; method supplied by the callback object.&lt;/p&gt; &lt;p&gt;The actual callback object is defined in another part of the code. The behavior of the callback object's &lt;code&gt;processMessage()&lt;/code&gt; method is unknown to &lt;code&gt;runAlways()&lt;/code&gt;. As long as the callback object publishes a method that conforms to the method signature defined by the pre-existing interface &lt;code&gt;KafkaMessageHandler&lt;/code&gt;, the code will execute without a problem. Putting the message processing behavior in a place different from where the actual message retrieval occurs makes the code more flexible overall. This is the essential benefit of using a callback object.&lt;/p&gt; &lt;p&gt;The work of polling the Kafka broker and processing retrieved messages is encapsulated in a try-catch block. The code will catch a &lt;code&gt;WakeupException&lt;/code&gt;, which is raised when an outside thread attempts to stop message consumption. The reason the&lt;code&gt;WakeupException&lt;/code&gt; is caught and checked is to make sure the call to stop message consumption came from the &lt;code&gt;SimpleConsumer&lt;/code&gt;'s &lt;code&gt;shutdown()&lt;/code&gt; method, as shown in the following listing.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; public void shutdown() throws Exception { closed.set(true); log.info(MessageHelper.getSimpleJSONObject("Shutting down consumer")); getKafkaConsumer().wakeup(); } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The private class variable named &lt;code&gt;closed&lt;/code&gt; is provided as a logical gate that gets “closed” from within the &lt;code&gt;shutdown()&lt;/code&gt; method. Only when the gate is closed by &lt;code&gt;shutdown()&lt;/code&gt; will the &lt;code&gt;WakeupException&lt;/code&gt; be allowed to execute from the call within &lt;code&gt;runAlways()&lt;/code&gt;. Remember, under the covers, throwing a &lt;code&gt;WakeupException&lt;/code&gt; has the effect of shutting down message consumption.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;closed&lt;/code&gt; variable is of type &lt;code&gt;AtomicBoolean&lt;/code&gt;. This type of variable provides a thread-safe way to represent state among threads. If for some reason an outside thread raises a &lt;code&gt;WakeupException&lt;/code&gt;, it will be ignored. As mentioned previously, only a &lt;code&gt;WakeupException&lt;/code&gt; that is raised by &lt;code&gt;shutdown()&lt;/code&gt; will be respected. It's fundamentally good housekeeping. In this case, the behavior of &lt;code&gt;shutdown()&lt;/code&gt; is to log a shutdown event. However, there are other graceful instructions that could be included in the &lt;code&gt;shutdown()&lt;/code&gt; behavior too.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;runAlways()&lt;/code&gt; method of the &lt;code&gt;SimpleConsumer&lt;/code&gt; class is the core means by which message consumption from a Kafka broker is implemented in the demonstration project. Now that you've reviewed the code, the next thing to do is to get it up and running.&lt;/p&gt; &lt;h2&gt;Getting Kafka up and running with Podman&lt;/h2&gt; &lt;p&gt;Before you can run the demonstration project, you need to have an instance of Kafka installed on the local computer on which the project's code will run. The easiest way to do this on a local machine is to use a Linux container.&lt;/p&gt; &lt;p&gt;The following is the command to get Kafka up and running in a terminal window on a computer that has Podman installed:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; podman run -it --name kafka-zkless -p 9092:9092 -e LOG_DIR=/tmp/logs quay.io/strimzi/kafka:latest-kafka-2.8.1-amd64 /bin/sh -c 'export CLUSTER_ID=$(bin/kafka-storage.sh random-uuid) &amp;&amp; bin/kafka-storage.sh format -t $CLUSTER_ID -c config/kraft/server.properties &amp;&amp; bin/kafka-server-start.sh config/kraft/server.properties' &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;To see if your system has Podman installed, type the following in a terminal window:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;which podman &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If Podman is installed, you'll see output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;/usr/bin/podman &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If the call to &lt;code&gt;which podman&lt;/code&gt; results in no return value, Podman is not installed.&lt;/p&gt; &lt;h2&gt;Getting Kafka up and running with Docker&lt;/h2&gt; &lt;p&gt;Below is the command to execute in a terminal window to get Kafka up and running on a Linux machine using Docker:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; docker run -it --name kafka-zkless -p 9092:9092 -e LOG_DIR=/tmp/logs quay.io/strimzi/kafka:latest-kafka-2.8.1-amd64 /bin/sh -c 'export CLUSTER_ID=$(bin/kafka-storage.sh random-uuid) &amp;&amp; bin/kafka-storage.sh format -t $CLUSTER_ID -c config/kraft/server.properties &amp;&amp; bin/kafka-server-start.sh config/kraft/server.properties' &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;To see if your system has Docker installed, type the following in a terminal window:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; which docker &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If Docker is installed, you'll see output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;/usr/local/bin/docker &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If the call to &lt;code&gt;which docker&lt;/code&gt; results in no return value, Docker is not installed.&lt;/p&gt; &lt;h2&gt;Installing Podman or Docker&lt;/h2&gt; &lt;p&gt;If you have neither Podman nor Docker installed on your computer, you need to install one of the container managers. Check out the documentation for &lt;a href="https://podman.io/getting-started/installation.html"&gt;Podman&lt;/a&gt; or &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker&lt;/a&gt; for installation instructions.&lt;/p&gt; &lt;h2&gt;Testing the demonstration project&lt;/h2&gt; &lt;p&gt;Run the following command from the top level of the directory where you installed the demonstration code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;maven test&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Executing this command will have the Maven framework run the unit tests defined in the &lt;code&gt;src/test/java/com/demo/kafka&lt;/code&gt; directory of the demonstration project.&lt;/p&gt; &lt;h2&gt;Running the demonstration project&lt;/h2&gt; &lt;p&gt;The demonstration project is made up of two parts. First, you need to run the producer to continuously emit messages that have random data. Then you need to run the consumer, which will retrieve those messages and process them. The processing behavior of the consumer is to log the contents of a retrieved message. Those log messages will appear in the file &lt;code&gt;logging.log&lt;/code&gt;, which will be in the root level of the demonstration application working folder.&lt;/p&gt; &lt;p&gt;The following sections describe how to get both the producer and consumer running. Remember: You must have an instance of Kafka up and running on the local machine in order for this all to work.&lt;/p&gt; &lt;h3&gt;Starting a continuously running producer&lt;/h3&gt; &lt;p&gt;In a new terminal window, go to the directory in which the demonstration code is installed and execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sh ./runproducer.sh mytopic&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You should see a steady stream of screen output. That's the log output of messages being sent to the topic named &lt;code&gt;mytopic&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Starting a continuously running consumer&lt;/h3&gt; &lt;p&gt;In another terminal window, go to the same directory and execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sh ./runconsumer.sh "mytopic"&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You should see a steady stream of screen output. That's the log output of messages being sent and retrieved from the topic named &lt;code&gt;mytopic&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;At this point, if you're running the demonstration project, you should have a producer emitting messages continuously and a consumer retrieving those messages. The following listing shows the log entries that &lt;code&gt;SimpleConsumer&lt;/code&gt; generates when messages are retrieved.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;2022-03-05 09:37:55 INFO KafkaMessageHandlerImpl:26 - {"bootstrapServers":"localhost:9092","topic":"mycooltopic","source":"com.demo.kafka.KafkaMessageHandlerImpl","message":"2djW70ABZM","key":"b506a6ac-b354-49cc-88f4-37e4b59b8b1a"} 2022-03-05 09:37:55 INFO KafkaMessageHandlerImpl:26 - {"bootstrapServers":"localhost:9092","topic":"mycooltopic","source":"com.demo.kafka.KafkaMessageHandlerImpl","message":"gPpzP6quCY","key":"2c09847f-037c-455d-845e-7a21b8e8912c"} &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Your work is now done!&lt;/p&gt; &lt;h2&gt;Putting it all together&lt;/h2&gt; &lt;p&gt;This has been a lengthy article, no doubt. Between the content and the demonstration code, we've covered a lot. You reviewed the purpose of Kafka. I described how the demonstration code is organized. I went into detail about how the client classes &lt;code&gt;KafkaProducer&lt;/code&gt; and &lt;code&gt;KafkaConsumer&lt;/code&gt; are used by their wrapper classes &lt;code&gt;SimpleProducer&lt;/code&gt; and &lt;code&gt;SimpleConsumer&lt;/code&gt;, respectively. I also discussed some of the object-oriented aspects of the demonstration code's structure, particularly around supporting the DRY principle.&lt;/p&gt; &lt;p&gt;As mentioned at the beginning of this article, the classes published by the Apache Kafka client package are intended to make programming against Kafka brokers an easier undertaking. Hopefully, the code and contents in this article have proven that assertion true.&lt;/p&gt; &lt;h2&gt;Learn more about Kafka at Red Hat&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/topics/integration/what-is-apache-kafka"&gt;What is Apache Kafka?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka on Kubernetes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/about/videos/event-driven-architecture-apache-kafka-and-openshift"&gt;Event driven architecture with Apache Kafka and OpenShift: How to use Kafka on Kubernetes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2018/10/29/how-to-run-kafka-on-openshift-the-enterprise-kubernetes-with-amq-streams"&gt;How to run Kafka on OpenShift, the enterprise Kubernetes, with AMQ Streams&lt;/a&gt;&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/02/how-create-kafka-consumers-and-producers-java" title="How to create Kafka consumers and producers in Java"&gt;How to create Kafka consumers and producers in Java&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-06-02T07:00:00Z</dc:date></entry></feed>
